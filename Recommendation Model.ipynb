{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Recommendation Engine\nOnline Challenge: Build A Recommendation Engine ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 1. Set Credentials for IBM cloud", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 49, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<ibmos2spark.osconfig.CloudObjectStorage at 0x2b77911ac1d0>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "## 2. Load Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2.1 Train data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+---------+--------+------------+---------+-------+---------+\n|CustomerID|InvoiceNo|Quantity| InvoiceDate|UnitPrice|Country|StockCode|\n+----------+---------+--------+------------+---------+-------+---------+\n|     27270|    27270|       7|1/12/10 8:26|     2.55|     PX|  85123AY|\n|     27270|    27270|       7|1/12/10 8:26|     3.39|     PX|   71053R|\n|     27270|    27270|       9|1/12/10 8:26|     2.75|     PX|  84406BH|\n|     27270|    27270|       7|1/12/10 8:26|     3.39|     PX|  84029GV|\n|     27270|    27270|       7|1/12/10 8:26|     3.39|     PX|  84029EX|\n|     27270|    27270|       2|1/12/10 8:26|     7.65|     PX|   22752G|\n|     27270|    27270|       7|1/12/10 8:26|     4.25|     PX|   21730R|\n|    427266|   427266|       3|1/12/10 8:35|     5.95|     PX|   21756Q|\n|    462735|   462735|      28|1/12/10 8:45|     3.75|     RA|   22728B|\n|    462735|   462735|      28|1/12/10 8:45|     3.75|     RA|   22727B|\n|    462735|   462735|      14|1/12/10 8:45|     3.75|     RA|   22726C|\n|    462735|   462735|      14|1/12/10 8:45|     0.85|     RA|   21724T|\n|    462735|   462735|      28|1/12/10 8:45|     0.65|     RA|   21883V|\n|    462735|   462735|      57|1/12/10 8:45|     0.85|     RA|   10002F|\n|    462735|   462735|      28|1/12/10 8:45|     1.25|     RA|   21791H|\n|    462735|   462735|      21|1/12/10 8:45|     2.95|     RA|   21035O|\n|    462735|   462735|      28|1/12/10 8:45|     2.95|     RA|   22326L|\n|    462735|   462735|      28|1/12/10 8:45|     1.95|     RA|   22629C|\n|    462735|   462735|      28|1/12/10 8:45|     1.95|     RA|   22659F|\n|    462735|   462735|      28|1/12/10 8:45|     1.95|     RA|   22631M|\n+----------+---------+--------+------------+---------+-------+---------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_train = spark.read\\\n    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n    .option('header', 'true')\\\n    .load(cos.url('train.csv', 'recommendationengine-donotdelete-pr-vknxjbiqabtc3s'))\n\ndf_train.createOrReplaceTempView('df_train')\ndf_train.show()"
        }, 
        {
            "source": "### 2.2 Test data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+----------+------------+---------+--------+---------+---------+\n|Country|CustomerID| InvoiceDate|InvoiceNo|Quantity|StockCode|UnitPrice|\n+-------+----------+------------+---------+--------+---------+---------+\n|     PX|    127269|1/12/10 8:28|   127269|       7|   22633V|     1.85|\n|     PX|    227268|1/12/10 8:34|   227268|      38|   84879M|     1.69|\n|     PX|    227268|1/12/10 8:34|   227268|       7|   22748P|      2.1|\n|     PX|    227268|1/12/10 8:34|   227268|       9|   22749K|     3.75|\n|     PX|    227268|1/12/10 8:34|   227268|       2|   22622G|     9.95|\n|     PX|    227268|1/12/10 8:34|   227268|       4|   48187N|     7.95|\n|     PX|    327267|1/12/10 8:34|   327267|       3|   22914W|     4.95|\n|     PX|    162738|1/12/10 9:02|   162738|       7|  85123AY|     2.55|\n|     PX|    162738|1/12/10 9:02|   162738|       7|   71053R|     3.39|\n|     PX|    162738|1/12/10 9:02|   162738|       9|  84406BH|     2.75|\n|     PX|    162738|1/12/10 9:02|   162738|       7|   37370Z|     1.06|\n|     PX|    162738|1/12/10 9:02|   162738|       7|   21871Y|     1.06|\n|     PX|    162738|1/12/10 9:02|   162738|       7|   21071P|     1.06|\n|     PX|    162738|1/12/10 9:02|   162738|       2|   82483P|     4.95|\n|     PX|    162738|1/12/10 9:02|   162738|       2|   22752G|     7.65|\n|     PX|    162738|1/12/10 9:02|   162738|       7|   21730R|     4.25|\n|     PX|    252747|1/12/10 9:45|   252747|       9|   21912V|     3.75|\n|     PX|    252747|1/12/10 9:45|   252747|      60|   22381J|     1.85|\n|     PX|    252747|1/12/10 9:45|   252747|       9|   22798I|     2.95|\n|     PX|    252747|1/12/10 9:45|   252747|       4|   22726C|     3.75|\n+-------+----------+------------+---------+--------+---------+---------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df_test = spark.read\\\n    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n    .option('header', 'true')\\\n    .load(cos.url('test.csv', 'recommendationengine-donotdelete-pr-vknxjbiqabtc3s'))\n\ndf_test.createOrReplaceTempView('df_test')\ndf_test.show()"
        }, 
        {
            "source": "## 3. Exploration", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3.1  Total row", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Total row of train data : 330575\nTotal row of test data : 103097\n"
                }
            ], 
            "source": "from pyspark.sql.functions import *\nprint('Total row of train data :',df_train.select(count('CustomerID')).head()[0])\n#df_train.select(count('CustomerID')).show()\nprint('Total row of test data :', df_test.select(count('CustomerID')).head()[0])\n#df_test.select(count('CustomerID')).show()"
        }, 
        {
            "source": "### 3.2 Total Customer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of customer in train data : 972\nNumber of customer in test data : 628\n"
                }
            ], 
            "source": "# Find number of customer in train & test data\ndf1 = spark.sql('SELECT CustomerID FROM df_train GROUP BY CustomerID')\ndf2 = spark.sql('SELECT CustomerID FROM df_test GROUP BY CustomerID')\nprint('Number of customer in train data :' , df1.select(count('CustomerID')).head()[0]) \n#df1.select(count('CustomerID')).show()\nprint('Number of customer in test data :' , df2.select(count('CustomerID')).head()[0])\n#df2.select(count('CustomerID')).show()"
        }, 
        {
            "source": "### 3.4 StockCode", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of StockCode : 3810\n+---------+----------------+\n|StockCode|count(StockCode)|\n+---------+----------------+\n|  85123AY|            1399|\n|   22423U|            1344|\n|  85099BJ|            1320|\n|   47566Y|            1049|\n|   20725N|            1001|\n|   22720A|             938|\n|   22197J|             906|\n|   84879M|             876|\n|   21212D|             866|\n|   22383K|             831|\n|   20727V|             820|\n|   22457C|             805|\n|   22469S|             788|\n|   23203B|             775|\n|   22386V|             769|\n|   22086O|             763|\n|   22960K|             757|\n|   20728Z|             750|\n|   22961C|             747|\n|   23298Q|             745|\n|   21931F|             741|\n|   22382I|             740|\n|   22411U|             736|\n|   22666I|             728|\n|   23209X|             715|\n|   22384W|             689|\n|   22699D|             688|\n|   23206T|             679|\n|   22727B|             674|\n|   22993E|             673|\n|   20726C|             667|\n|   22178V|             660|\n|   22697A|             657|\n|   82482D|             656|\n|   20724F|             651|\n|   23084M|             644|\n|   22077P|             632|\n|   22726C|             624|\n|   23199M|             618|\n|   22139Q|             616|\n|   21080R|             600|\n|   22470E|             593|\n|  85099CQ|             591|\n|   21034O|             590|\n|   22138G|             583|\n|   84946F|             577|\n|   23201S|             576|\n|   21790Y|             574|\n|   22629C|             570|\n|   23301G|             570|\n+---------+----------------+\nonly showing top 50 rows\n\n"
                }
            ], 
            "source": "# Find number of StockCode\ndf = spark.sql('SELECT StockCode, COUNT(StockCode) FROM df_train GROUP BY StockCode ORDER BY COUNT(StockCode) DESC')\nprint('Number of StockCode :' , df.select(count('StockCode')).head()[0]) \ndf.show(50)"
        }, 
        {
            "source": "## 4. Create New Data  to Define Group of Customer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Select frist 30 popular item", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# List of Customer\ndf_ID = spark.sql('SELECT CustomerID FROM df_train GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"85123AY\" by CustomerID\ndf1 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f1 FROM df_train WHERE StockCode = \"85123AY\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22423U\" by CustomerID\ndf2 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f2 FROM df_train WHERE StockCode = \"22423U\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"85099BJ\" by CustomerID\ndf3 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f3 FROM df_train WHERE StockCode = \"85099BJ\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"47566Y\" by CustomerID\ndf4 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f4 FROM df_train WHERE StockCode = \"47566Y\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"20725N\" by CustomerID\ndf5 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f5 FROM df_train WHERE StockCode = \"20725N\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"22720A\" by CustomerID\ndf6 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f6 FROM df_train WHERE StockCode = \"22720A\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22197J\" by CustomerID\ndf7 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f7 FROM df_train WHERE StockCode = \"22197J\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"84879M\" by CustomerID\ndf8 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f8 FROM df_train WHERE StockCode = \"84879M\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"21212D\" by CustomerID\ndf9 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f9 FROM df_train WHERE StockCode = \"21212D\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22383K\" by CustomerID\ndf10 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f10 FROM df_train WHERE StockCode = \"22383K\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"20727V\" by CustomerID\ndf11 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f11 FROM df_train WHERE StockCode = \"20727V\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22457C\" by CustomerID\ndf12 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f12 FROM df_train WHERE StockCode = \"22457C\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22469S\" by CustomerID\ndf13 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f13 FROM df_train WHERE StockCode = \"22469S\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23203B\" by CustomerID\ndf14 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f14 FROM df_train WHERE StockCode = \"23203B\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22386V\" by CustomerID\ndf15 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f15 FROM df_train WHERE StockCode = \"22386V\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22086O\" by CustomerID\ndf16 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f16 FROM df_train WHERE StockCode = \"22086O\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22960K\" by CustomerID\ndf17 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f17 FROM df_train WHERE StockCode = \"22960K\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"20728Z\" by CustomerID\ndf18 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f18 FROM df_train WHERE StockCode = \"20728Z\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22961C\" by CustomerID\ndf19 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f19 FROM df_train WHERE StockCode = \"22961C\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23298Q\" by CustomerID\ndf20 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f20 FROM df_train WHERE StockCode = \"23298Q\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"21931F\" by CustomerID\ndf21 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f21 FROM df_train WHERE StockCode = \"21931F\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22382I\" by CustomerID\ndf22 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f22 FROM df_train WHERE StockCode = \"22382I\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22411U\" by CustomerID\ndf23 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f23 FROM df_train WHERE StockCode = \"22411U\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22666I\" by CustomerID\ndf24 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f24 FROM df_train WHERE StockCode = \"22666I\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23209X\" by CustomerID\ndf25 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f25 FROM df_train WHERE StockCode = \"23209X\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22384W\" by CustomerID\ndf26 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f26 FROM df_train WHERE StockCode = \"22384W\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22699D\" by CustomerID\ndf27 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f27 FROM df_train WHERE StockCode = \"22699D\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23206T\" by CustomerID\ndf28 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f28 FROM df_train WHERE StockCode = \"23206T\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22727B\" by CustomerID\ndf29 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f29 FROM df_train WHERE StockCode = \"22727B\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22993E\" by CustomerID\ndf30 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f30 FROM df_train WHERE StockCode = \"22993E\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n\n    "
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Check Number of CustomerID and Varaible\nNumber of customer ; 972\n"
                }
            ], 
            "source": "new_df = df_ID.join(df1, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df2, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df3, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df4, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df5, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df6, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df7, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df8, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df9, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df10, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df11, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df12, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df13, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df14, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df15, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df16, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df17, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df18, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df19, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df20, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df21, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df22, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df23, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df24, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df25, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df26, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df27, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df28, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df29, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df30, 'CustomerID', 'left_outer')\n\n# Replace null with 0\nnew_df = new_df.na.fill(0)\nprint('Check Number of CustomerID and Varaible')\nprint('Number of customer ;', new_df.select(count('CustomerID')).head()[0])\n"
        }, 
        {
            "source": "## 5. Clustering", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.clustering import KMeansModel\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import PCA\n"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "assembler = VectorAssembler(inputCols=['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10',\\\n                                       'f11','f12','f13','f14','f15','f16','f17','f18','f19','f20',\\\n                                       'f21','f22','f23','f24','f25','f26','f27','f28','f29','f30'],\\\n                            outputCol=\"features\")\nnormalizer = Normalizer(inputCol='features' , outputCol='features_norm', p=1.0)"
        }, 
        {
            "source": "### 5.1 Find K use PCA ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 208, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# These line use to find PCA\npca = PCA(k=3, inputCol= 'features', outputCol=\"pcaFeatures\")\npipeline_PCA = Pipeline(stages=[assembler, normalizer, pca])\nmodel_PCA = pipeline_PCA.fit(new_df)\n\nresult_PCA = model_PCA.transform(new_df).select(\"pcaFeatures\")\nresult_PCA.show(truncate=False)\n"
        }, 
        {
            "source": "### 5.2 K-mean ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+-----+\n|cluster|count|\n+-------+-----+\n|     28|   16|\n|     27|  163|\n|     26|    2|\n|     12|    2|\n|     22|    9|\n|      1|  569|\n|     13|    1|\n|     16|    1|\n|      6|    1|\n|      3|    1|\n|     20|   32|\n|      5|    1|\n|     19|    3|\n|     15|   11|\n|     17|   19|\n|      9|    2|\n|      4|    4|\n|      8|   19|\n|     23|   65|\n|      7|    7|\n|     10|    2|\n|     25|   26|\n|     24|    2|\n|     29|    4|\n|     21|    1|\n|     11|    1|\n|     14|    1|\n|      2|    3|\n|      0|    1|\n|     18|    3|\n+-------+-----+\n\n"
                }
            ], 
            "source": "# setK = \nkmeans = KMeans(maxIter=20, predictionCol=\"cluster\").setK(30).setSeed(1)  # Initialize model\npipeline_Kmean = Pipeline(stages=[assembler, normalizer, kmeans])\nmodel_Kmean = pipeline_Kmean.fit(new_df)\n\nresult_Kmean = model_Kmean.transform(new_df)\nresult_Kmean.groupby('cluster').count().show(30)"
        }, 
        {
            "source": "## 6. Save New train\nnew train is train with cluster (save to data asset)\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 43, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 1. Need to transform data frame \nimport pandas as pd\ndf_cluster = result_Kmean['CustomerID', 'cluster']\nnew_train = df_train.join(df_cluster, 'CustomerID', 'left_outer')\nnew_train = new_train.toPandas()\n"
        }, 
        {
            "execution_count": 47, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 47, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{'asset_id': '3e96d903-9d81-4e4f-a208-b89879e457d0',\n 'bucket_name': 'recommendationengine-donotdelete-pr-vknxjbiqabtc3s',\n 'file_name': 'new_train.csv',\n 'message': 'File saved to project storage.'}"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "## 7. Make New Test ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 7.1 Load test data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 50, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_test = spark.read\\\n    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n    .option('header', 'true')\\\n    .load(cos.url('test.csv', 'recommendationengine-donotdelete-pr-vknxjbiqabtc3s'))\n\ndf_test.createOrReplaceTempView('df_test')"
        }, 
        {
            "source": "### 7.2 Transform data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 51, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# List of Customer\ndf_ID = spark.sql('SELECT CustomerID FROM df_test GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"85123AY\" by CustomerID\ndf1 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f1 FROM df_test WHERE StockCode = \"85123AY\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22423U\" by CustomerID\ndf2 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f2 FROM df_test WHERE StockCode = \"22423U\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"85099BJ\" by CustomerID\ndf3 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f3 FROM df_test WHERE StockCode = \"85099BJ\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"47566Y\" by CustomerID\ndf4 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f4 FROM df_test WHERE StockCode = \"47566Y\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"20725N\" by CustomerID\ndf5 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f5 FROM df_test WHERE StockCode = \"20725N\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n \n# Purchase Product \"22720A\" by CustomerID\ndf6 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f6 FROM df_test WHERE StockCode = \"22720A\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22197J\" by CustomerID\ndf7 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f7 FROM df_test WHERE StockCode = \"22197J\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"84879M\" by CustomerID\ndf8 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f8 FROM df_test WHERE StockCode = \"84879M\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"21212D\" by CustomerID\ndf9 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f9 FROM df_test WHERE StockCode = \"21212D\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22383K\" by CustomerID\ndf10 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f10 FROM df_test WHERE StockCode = \"22383K\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"20727V\" by CustomerID\ndf11 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f11 FROM df_test WHERE StockCode = \"20727V\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22457C\" by CustomerID\ndf12 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f12 FROM df_test WHERE StockCode = \"22457C\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22469S\" by CustomerID\ndf13 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f13 FROM df_test WHERE StockCode = \"22469S\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23203B\" by CustomerID\ndf14 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f14 FROM df_test WHERE StockCode = \"23203B\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22386V\" by CustomerID\ndf15 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f15 FROM df_test WHERE StockCode = \"22386V\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22086O\" by CustomerID\ndf16 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f16 FROM df_test WHERE StockCode = \"22086O\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22960K\" by CustomerID\ndf17 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f17 FROM df_test WHERE StockCode = \"22960K\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"20728Z\" by CustomerID\ndf18 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f18 FROM df_test WHERE StockCode = \"20728Z\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22961C\" by CustomerID\ndf19 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f19 FROM df_test WHERE StockCode = \"22961C\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23298Q\" by CustomerID\ndf20 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f20 FROM df_test WHERE StockCode = \"23298Q\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"21931F\" by CustomerID\ndf21 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f21 FROM df_test WHERE StockCode = \"21931F\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22382I\" by CustomerID\ndf22 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f22 FROM df_test WHERE StockCode = \"22382I\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22411U\" by CustomerID\ndf23 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f23 FROM df_test WHERE StockCode = \"22411U\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22666I\" by CustomerID\ndf24 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f24 FROM df_test WHERE StockCode = \"22666I\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23209X\" by CustomerID\ndf25 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f25 FROM df_test WHERE StockCode = \"23209X\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22384W\" by CustomerID\ndf26 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f26 FROM df_test WHERE StockCode = \"22384W\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22699D\" by CustomerID\ndf27 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f27 FROM df_test WHERE StockCode = \"22699D\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"23206T\" by CustomerID\ndf28 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f28 FROM df_test WHERE StockCode = \"23206T\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22727B\" by CustomerID\ndf29 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f29 FROM df_test WHERE StockCode = \"22727B\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n# Purchase Product \"22993E\" by CustomerID\ndf30 = spark.sql('SELECT CustomerID, SUM(Quantity) AS f30 FROM df_test WHERE StockCode = \"22993E\" GROUP BY CustomerID ORDER BY CustomerID ASC')\n\n"
        }, 
        {
            "execution_count": 52, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Check Number of CustomerID and Varaible\nNumber of customer ; 628\n"
                }
            ], 
            "source": "new_df = df_ID.join(df1, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df2, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df3, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df4, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df5, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df6, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df7, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df8, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df9, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df10, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df11, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df12, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df13, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df14, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df15, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df16, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df17, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df18, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df19, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df20, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df21, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df22, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df23, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df24, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df25, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df26, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df27, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df28, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df29, 'CustomerID', 'left_outer')\nnew_df = new_df.join(df30, 'CustomerID', 'left_outer')\n\n# Replace null with 0\nnew_df = new_df.na.fill(0)\nprint('Check Number of CustomerID and Varaible')\nprint('Number of customer ;', new_df.select(count('CustomerID')).head()[0])\n\n"
        }, 
        {
            "source": "### 7.3 Clustering", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 53, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+-----+\n|cluster|count|\n+-------+-----+\n|     28|    7|\n|     27|   48|\n|     12|    1|\n|     22|    7|\n|      1|  492|\n|      6|    1|\n|     20|   16|\n|     17|    6|\n|      8|    2|\n|     23|   30|\n|      7|    4|\n|     10|    1|\n|     25|    6|\n|     14|    1|\n|      2|    1|\n|      0|    3|\n|     18|    2|\n+-------+-----+\n\n"
                }
            ], 
            "source": "result_Kmean = model_Kmean.transform(new_df)\nresult_Kmean.groupby('cluster').count().show(30)"
        }, 
        {
            "source": "### 7.4 Save New Test", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 1. Need to transform data frame \nimport pandas as pd\ndf_cluster = result_Kmean['CustomerID', 'cluster']\nnew_test = df_test.join(df_cluster, 'CustomerID', 'left_outer')\nnew_test = new_test.toPandas()\n"
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 56, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{'asset_id': '72e84155-caf5-4754-9504-ae4632cce35b',\n 'bucket_name': 'recommendationengine-donotdelete-pr-vknxjbiqabtc3s',\n 'file_name': 'new_test.csv',\n 'message': 'File saved to project storage.'}"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "## 8. Make Recommend Product for each Cluster ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DataFrame[CustomerID: string, f1: double, f2: double, f3: double, f4: double, f5: double, f6: double, f7: double, f8: double, f9: double, f10: double, f11: double, f12: double, f13: double, f14: double, f15: double, f16: double, f17: double, f18: double, f19: double, f20: double, f21: double, f22: double, f23: double, f24: double, f25: double, f26: double, f27: double, f28: double, f29: double, f30: double, features: vector, features_norm: vector, cluster: int]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "result_Kmean"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_cluster = result_Kmean['CustomerID', 'cluster']"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_train_1 = df_train.join(df_cluster, 'CustomerID', 'left_outer')"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+---------+--------+--------------+---------+-------+---------+-------+\n|CustomerID|InvoiceNo|Quantity|   InvoiceDate|UnitPrice|Country|StockCode|cluster|\n+----------+---------+--------+--------------+---------+-------+---------+-------+\n|    147348|   147348|       9| 3/12/10 14:30|     1.65|     GJ|   22331U|     27|\n|    147348|   147348|      14| 3/12/10 14:30|      2.1|     GJ|   22865Y|     27|\n|    147348|   147348|       4| 3/12/10 14:30|      8.5|     GJ|   22171T|     27|\n|    147348|   147348|       4| 3/12/10 14:30|     5.95|     GJ|   22926R|     27|\n|    147348|   147348|       3| 3/12/10 14:30|     4.95|     GJ|   22914W|     27|\n|    147348|   147348|      14| 3/12/10 14:30|     1.69|     GJ|  84849DL|     27|\n|    147348|   147348|       7| 3/12/10 14:30|     2.95|     GJ|   22445C|     27|\n|    147348|   147348|      12| 3/12/10 14:30|     0.85|     GJ|   16016H|     27|\n|    147348|   147348|      -1|  6/1/11 18:08|    10.95|     PX|   21843C|     27|\n|    147348|   147348|      -4|  6/1/11 18:08|     2.95|     PX|   22699D|     27|\n|    147348|   147348|      -1|  6/1/11 18:08|     3.25|     PX|   71477B|     27|\n|    147348|   147348|      -1|  6/1/11 18:08|     2.95|     PX|   22457C|     27|\n|    147348|   147348|       4|18/01/11 14:29|     5.79|     PX|   22111I|     27|\n|    147348|   147348|       1|18/01/11 14:29|    12.46|     PX|   22111I|     27|\n|    147348|   147348|       8|18/01/11 14:29|     5.79|     PX|   22112V|     27|\n|    147348|   147348|       7|18/01/11 14:29|     6.63|     PX|   22326L|     27|\n|    147348|   147348|       3|18/01/11 14:29|     6.63|     PX|   22327P|     27|\n|    147348|   147348|       2|18/01/11 14:29|     4.13|     PX|   22328M|     27|\n|    147348|   147348|      93|18/01/11 14:29|     2.46|     PX|   22355U|     27|\n|    147348|   147348|      21|18/01/11 14:29|     2.46|     PX|   22356T|     27|\n|    147348|   147348|       2|18/01/11 14:29|     4.96|     PX|   22384W|     27|\n|    147348|   147348|       7|18/01/11 14:29|     4.13|     PX|   22411U|     27|\n|    147348|   147348|       4|18/01/11 14:29|     5.79|     PX|   22725X|     27|\n|    147348|   147348|       1|18/01/11 14:29|     5.79|     PX|   22729W|     27|\n|    147348|   147348|       2|18/01/11 14:29|     5.79|     PX|   22734C|     27|\n|    147348|   147348|       1|18/01/11 14:29|     4.96|     PX|   22834J|     27|\n|    147348|   147348|       1|18/01/11 14:29|     7.46|     PX|   22835R|     27|\n|    147348|   147348|       3|18/01/11 14:29|     5.79|     PX|   47580T|     27|\n|    147348|   147348|       1|18/01/11 14:29|    12.46|     PX|   48187N|     27|\n|    147348|   147348|       2|18/01/11 14:29|     8.29|     PX|  84029GV|     27|\n|    147348|   147348|       2|18/01/11 14:29|     1.63|     PX|   20675E|     27|\n|    147348|   147348|       1|18/01/11 14:29|     1.63|     PX|   20676O|     27|\n|    147348|   147348|       3|18/01/11 14:29|     1.63|     PX|   20677D|     27|\n|    147348|   147348|       2|18/01/11 14:29|    16.63|     PX|   20685H|     27|\n|    147348|   147348|       7|18/01/11 14:29|     3.29|     PX|   20718A|     27|\n|    147348|   147348|      32|18/01/11 14:29|     2.46|     PX|   20719E|     27|\n|    147348|   147348|      19|18/01/11 14:29|     2.46|     PX|   20723U|     27|\n|    147348|   147348|      44|18/01/11 14:29|     2.46|     PX|   20724F|     27|\n|    147348|   147348|      14|18/01/11 14:29|     4.96|     PX|   20726C|     27|\n|    147348|   147348|       3|18/01/11 14:29|     4.96|     PX|   20727V|     27|\n|    147348|   147348|       4|18/01/11 14:29|     4.13|     PX|   20728Z|     27|\n|    147348|   147348|       4|18/01/11 14:29|     1.63|     PX|   20734Z|     27|\n|    147348|   147348|       3|18/01/11 14:29|     2.46|     PX|   21078I|     27|\n|    147348|   147348|       1|18/01/11 14:29|     2.46|     PX|   21080R|     27|\n|    147348|   147348|      13|18/01/11 14:29|     1.25|     PX|   21215Q|     27|\n|    147348|   147348|       2|18/01/11 14:29|     1.63|     PX|   21238G|     27|\n|    147348|   147348|       2|18/01/11 14:29|     1.63|     PX|   21239K|     27|\n|    147348|   147348|       1|18/01/11 14:29|     1.63|     PX|   21240N|     27|\n|    147348|   147348|       4|18/01/11 14:29|     3.29|     PX|   21244H|     27|\n|    147348|   147348|       3|18/01/11 14:29|     2.46|     PX|   21327L|     27|\n+----------+---------+--------+--------------+---------+-------+---------+-------+\nonly showing top 50 rows\n\n"
                }
            ], 
            "source": "df_train_1.show(50)"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------------+\n|count(CustomerID)|\n+-----------------+\n|           330575|\n+-----------------+\n\n"
                }
            ], 
            "source": "df_train_1.select(count('CustomerID')).show()"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_train_1.createOrReplaceTempView('df_train_1')"
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 33, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DataFrame[CustomerID: string, InvoiceNo: string, Quantity: string, InvoiceDate: string, UnitPrice: string, Country: string, StockCode: string, cluster: int]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_train_1"
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---------+--------+\n|StockCode|cluster0|\n+---------+--------+\n|   84077K| 11532.0|\n|   16014F|  7964.0|\n|   22492E|  6546.0|\n|   15036Z|  6481.0|\n|   22616B|  5326.0|\n|   17003M|  5148.0|\n|   22178V|  4771.0|\n|   84879M|  4710.0|\n|   21703M|  4568.0|\n|   23084M|  4383.0|\n|   84946F|  4362.0|\n|   21790Y|  4128.0|\n|   21212D|  4082.0|\n|   22086O|  3974.0|\n|   84945I|  3947.0|\n|   84347V|  3901.0|\n|   22065D|  3631.0|\n|  85123AY|  3600.0|\n|   21915K|  3475.0|\n|   21977R|  3468.0|\n+---------+--------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "cluster = '1'\nrecommend = spark.sql('SELECT StockCode, SUM(Quantity) AS cluster0 FROM df_train_1 WHERE cluster = '+ cluster + ' GROUP BY StockCode ORDER BY SUM(Quantity) DESC')\nrecommend.show()"
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "StockCode_list = df.select('StockCode').collect()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": 132, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 132, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'85123AY'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "StockCode_list[0].StockCode"
        }, 
        {
            "execution_count": 135, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a= [row.StockCode for row in StockCode_list]"
        }, 
        {
            "execution_count": 136, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 136, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['85123AY',\n '22423U',\n '85099BJ',\n '47566Y',\n '20725N',\n '22720A',\n '22197J',\n '84879M',\n '21212D',\n '22383K',\n '20727V',\n '22457C',\n '22469S',\n '23203B',\n '22386V',\n '22086O',\n '22960K',\n '20728Z',\n '22961C',\n '23298Q',\n '21931F',\n '22382I',\n '22411U',\n '22666I',\n '23209X',\n '22384W',\n '22699D',\n '23206T',\n '22727B',\n '22993E',\n '20726C',\n '22178V',\n '22697A',\n '82482D',\n '20724F',\n '23084M',\n '22077P',\n '22726C',\n '23199M',\n '22139Q',\n '21080R',\n '22470E',\n '85099CQ',\n '21034O',\n '22138G',\n '84946F',\n '23201S',\n '21790Y',\n '23301G',\n '22629C',\n '23245W',\n '21929W',\n '20712E',\n '22355U',\n '21232B',\n '23202Z',\n '20914Q',\n '20719E',\n '22112V',\n '21977R',\n '22722E',\n '22630B',\n '82494LP',\n '85099FC',\n '22910D',\n '21731V',\n '21181K',\n '84991R',\n '21928E',\n '23355Z',\n '22090X',\n '22698C',\n '22111I',\n '21754X',\n '21175V',\n '22558H',\n '21733F',\n '23207R',\n '22728B',\n '84978X',\n '22356T',\n '23300F',\n '22379G',\n '20723U',\n '23344R',\n '22326L',\n '22624O',\n '21930O',\n '22659F',\n '22467G',\n '22662G',\n '22554P',\n '21166I',\n '22966E',\n '20685H',\n '82484O',\n '20713F',\n '22907C',\n '85152E',\n '22385P',\n '23208E',\n '21213V',\n '21915K',\n '22909Z',\n '84378P',\n '23284M',\n '79321V',\n '21755A',\n '22114V',\n '23321O',\n '21485W',\n '82583N',\n '22952P',\n '20972O',\n '23307D',\n '22865Y',\n '84380W',\n '22551F',\n '22621P',\n '22835R',\n '21559P',\n '22969Z',\n '82580N',\n '21914B',\n '84992D',\n '23200Y',\n '21791H',\n '82486S',\n '22776E',\n '48138T',\n '84755J',\n '23204B',\n '84692Z',\n '21975B',\n '22557Z',\n '21889K',\n '22083G',\n '48194E',\n '22367K',\n '22113T',\n '20718A',\n '22144E',\n '23322J',\n '82483P',\n '21843C',\n '22328M',\n '22556N',\n '48187N',\n '22730Z',\n '22866X',\n '22625P',\n '22553Q',\n '84970SL',\n '23293D',\n '22900U',\n '84077K',\n '23240L',\n '21314Q',\n '20711L',\n '22219N',\n '22867O',\n '85066U',\n '21479J',\n '22577B',\n '21936G',\n '20971S',\n '21231K',\n '22149E',\n '23343M',\n '22578A',\n '22766P',\n '84836J',\n '21481J',\n '22555O',\n '22427Q',\n '22029R',\n '84375E',\n '84596BQ',\n '22173N',\n '23356F',\n '22568A',\n '21174F',\n '82600H',\n '84988Y',\n '21935J',\n '22616B',\n '22189I',\n '15036Z',\n '23263V',\n '22456H',\n '21430A',\n '23198Y',\n '22417M',\n '22998M',\n '21094A',\n '22595D',\n '21523L',\n '21155Z',\n '22633V',\n '22424F',\n '22141S',\n '21621T',\n '22652D',\n '22147L',\n '22667K',\n '22804V',\n '22620B',\n '21669G',\n '22352Z',\n '22381J',\n '22749K',\n '23243L',\n '23205F',\n '35970L',\n '22196G',\n '22193B',\n '21154F',\n '20676O',\n '22501Q',\n '22502M',\n '22561Z',\n '21172S',\n '22084R',\n '22619G',\n '22150K',\n '22507V',\n '84029GV',\n '85150Z',\n '21668N',\n '21932M',\n '84997DC',\n '21169G',\n '22759Z',\n '84029EX',\n '22191G',\n '84347V',\n '21216B',\n '21524I',\n '15056ND',\n '21165F',\n '22988D',\n '21891R',\n '21210C',\n '22729W',\n '22489J',\n '22951B',\n '84970LF',\n '22617A',\n '22748P',\n '21908X',\n '22725X',\n '23295P',\n '82581P',\n '85049EP',\n '21892U',\n '22661W',\n '22734C',\n '22663L',\n '23182N',\n '21672A',\n '22692J',\n '21218O',\n '22750S',\n '21992O',\n '21933N',\n '22771L',\n '23309L',\n '47591DY',\n '47590BU',\n '23493H',\n '22488P',\n '22745M',\n '23319Q',\n '21484N',\n '84596FA',\n '21985C',\n '22487O',\n '23328X',\n '20717I',\n '22371T',\n '23173W',\n '22983O',\n '22844J',\n '82582N',\n '21993R',\n '22082A',\n '23103F',\n '71459J',\n '23170S',\n '23313G',\n '22646J',\n '22413V',\n '22585G',\n '22694S',\n '21934D',\n '21912V',\n '23439A',\n '22429T',\n '22908G',\n '47590AN',\n '21558V',\n '22499N',\n '48184N',\n '22996P',\n '21976M',\n '21035O',\n '22632O',\n '22171T',\n '21238G',\n '22570W',\n '23108J',\n '21673M',\n '22212M',\n '21086B',\n '21670S',\n '23296Z',\n '23308G',\n '22847W',\n '22142B',\n '22560W',\n '22979P',\n '22631M',\n '23318Z',\n '21137F',\n '22795L',\n '22989L',\n '20749K',\n '21156K',\n '22297H',\n '22665A',\n '21770E',\n '47566BG',\n '22992G',\n '22041A',\n '21871Y',\n '21671K',\n '22492E',\n '22898W',\n '20750C',\n '84997BG',\n '85053B',\n '48185J',\n '22962X',\n '21121C',\n '21990F',\n '21888F',\n '21136P',\n '21535E',\n '21326V',\n '22627E',\n '84947R',\n '21982Y',\n '22195J',\n '22579W',\n '23256O',\n '22834J',\n '22645V',\n '85014BD',\n '22169U',\n '23171H',\n '23168M',\n '23294M',\n '22549T',\n '21900A',\n '23077P',\n '22752G',\n '22607L',\n '22964E',\n '22037V',\n '84945I',\n '82552M',\n '20975Y',\n '21217W',\n '22940N',\n '21533B',\n '22348L',\n '22464B',\n '22637S',\n '21989J',\n '22027K',\n '22273M',\n '20974J',\n '22079R',\n '23188L',\n '23241W',\n '23581R',\n '22151L',\n '82551H',\n '21877C',\n '21876I',\n '22569R',\n '21122K',\n '23275I',\n '22161W',\n '21907O',\n '21916D',\n '21974Q',\n '84030EZ',\n '71053R',\n '21918L',\n '23351C',\n '23169O',\n '23236J',\n '22791E',\n '20973S',\n '23266U',\n '22192Y',\n '22296X',\n '71477B',\n '20979K',\n '22087E',\n '22851J',\n '23349J',\n '20677D',\n '23354R',\n '23320Z',\n '84032AK',\n '23265T',\n '23264K',\n '84997CQ',\n '23583L',\n '22418R',\n '23192L',\n '22654J',\n '22899Q',\n '15056BLH',\n '22963K',\n '23311K',\n '23353X',\n '23174D',\n '20675E',\n '85049AF',\n '23175H',\n '21328B',\n '22746G',\n '22333G',\n '84987W',\n '21868Y',\n '21240N',\n '22784C',\n '20679N',\n '22668V',\n '22174M',\n '22739W',\n '23503O',\n '22508O',\n '84949O',\n '21385E',\n '23247Z',\n '22158K',\n '22895J',\n '22723M',\n '23196E',\n '21498M',\n '84536AK',\n '23350Q',\n '22896Q',\n '21239K',\n '20969R',\n '82578R',\n '21494K',\n '20983Y',\n '21506J',\n '22605W',\n '23234A',\n '22980U',\n '21874B',\n '22294U',\n '22970T',\n '21242V',\n '22845W',\n '21429E',\n '21340G',\n '23332P',\n '23254E',\n '22431K',\n '22693N',\n '21901T',\n '23165S',\n '22768C',\n '22110N',\n '22941E',\n '21078I',\n '72741B',\n '23126J',\n '23323K',\n '22227K',\n '22271R',\n '22131I',\n '22378H',\n '21756Q',\n '23109C',\n '22380N',\n '21380E',\n '22505F',\n '85014AL',\n '22840S',\n '22712N',\n '20992N',\n '22751U',\n '22274N',\n '20754G',\n '22361C',\n '22846O',\n '22327P',\n '48188P',\n '22208X',\n '23082N',\n '21899D',\n '23172Q',\n '21703M',\n '21329X',\n '22991T',\n '22596T',\n '22644U',\n '47559BK',\n '23191Z',\n '85199SA',\n '21810L',\n '22563R',\n '16237N',\n '23144I',\n '23076Z',\n '85048L',\n '23238P',\n '22721T',\n '21980L',\n '22179E',\n '22670P',\n '21098K',\n '85049GC',\n '22817Z',\n '84032BE',\n '22419N',\n '21259G',\n '22738Y',\n '23367M',\n '21116Z',\n '23110A',\n '23052I',\n '22109D',\n '23570W',\n '22690X',\n '21164D',\n '21179M',\n '22602D',\n '22301J',\n '22906G',\n '22904F',\n '23329B',\n '22798I',\n '21257P',\n '21041U',\n '84832I',\n '23306Z',\n '84406BH',\n '23534D',\n '22622G',\n '20981E',\n '23184T',\n '23078Y',\n '22716A',\n '22754C',\n '22819Q',\n '22916M',\n '21033B',\n '23112V',\n '23100G',\n '22064T',\n '20978G',\n '22978F',\n '20982C',\n '23147S',\n '21509I',\n '84050K',\n '22917U',\n '22156L',\n '22737B',\n '82567U',\n '22918F',\n '22919F',\n '84997AH',\n '21981M',\n '21108Z',\n '21428B',\n '23231J',\n '21811V',\n '23333X',\n '21327L',\n '23166P',\n '22920I',\n '22432D',\n '23399Y',\n '22772B',\n '23080Z',\n '22241L',\n '22816Q',\n '22435K',\n '22303W',\n '22494U',\n '21937S',\n '21411J',\n '22775S',\n '23571G',\n '20970L',\n '22653U',\n '23389U',\n '22571W',\n '84792X',\n '21507E',\n '23118T',\n '23012H',\n '22358N',\n '22815Y',\n '23582G',\n '21714L',\n '51014AW',\n '22170J',\n '22028K',\n '22065D',\n '23000P',\n '22534C',\n '21592F',\n '17003M',\n '23197D',\n '21787C',\n '23005J',\n '22089I',\n '23158A',\n '23395C',\n '23148U',\n '23393Y',\n '22088I',\n '20828M',\n '20961S',\n '22995T',\n '23232F',\n '22580V',\n '23146P',\n '22781B',\n '23283I',\n '22030B',\n '22971A',\n '22818I',\n '23368D',\n '23154P',\n '22329R',\n '21531U',\n '21539P',\n '16161PA',\n '85227H',\n '23177K',\n '21658U',\n '21124F',\n '23230A',\n '22841B',\n '23480U',\n '23330V',\n '22997O',\n '23089K',\n '21704M',\n '22855Q',\n '22767S',\n '23167J',\n '23229V',\n '21561W',\n '23310X',\n '22741O',\n '23083I',\n '22360Q',\n '48111G',\n '23302F',\n '84510AH',\n '35961N',\n '22747N',\n '22755K',\n '22437B',\n '22441B',\n '21381P',\n '22965Z',\n '35471DY',\n '23397R',\n '21786G',\n '22897R',\n '21746N',\n '22796M',\n '22398K',\n '22530D',\n '47593BL',\n '20682A',\n '22295V',\n '21890Y',\n '21497K',\n '22076Q',\n '85049CP',\n '23382B',\n '20963G',\n '23176Z',\n '20668K',\n '22634D',\n '22614B',\n '21967J',\n '22921W',\n '84969T',\n '84509AK',\n '21880B',\n '22115J',\n '22335Z',\n '21955E',\n '22669U',\n '20751D',\n '22943F',\n '21519A',\n '84596GP',\n '22626Q',\n '22491V',\n '22676W',\n '22972W',\n '22302N',\n '22567D',\n '22636Z',\n '22672E',\n '23396G',\n '23352S',\n '22814E',\n '23127K',\n '23013O',\n '22713C',\n '22736S',\n '22222C',\n '22994B',\n '22366J',\n '21745D',\n '22550H',\n '23292K',\n '23504V',\n '23159Q',\n '21500P',\n '21313C',\n '22968N',\n '21243M',\n '22548G',\n '22433B',\n '22743K',\n '22493I',\n '21917S',\n '22677N',\n '20996R',\n '22024W',\n '23535V',\n '21390P',\n '22623L',\n '75049LA',\n '23497Q',\n '22422Q',\n '22792D',\n '23101Y',\n '22562M',\n '22574T',\n '23286O',\n '22800V',\n '48129Y',\n '21716W',\n '21991R',\n '22839T',\n '47504KA',\n '21623S',\n '22300C',\n '85132CX',\n '22426J',\n '21499T',\n '22999A',\n '21878P',\n '22593E',\n '22377W',\n '22957K',\n '22982D',\n '22926R',\n '21090G',\n '10133M',\n '21700U',\n '23210F',\n '21832G',\n '23494O',\n '23014Q',\n '22430D',\n '22801J',\n '23163U',\n '21984W',\n '21715T',\n '23290W',\n '22421F',\n '23050V',\n '85184CK',\n '23004D',\n '22610S',\n '21718N',\n '22649G',\n '22485Z',\n '22374J',\n '22055T',\n '23035A',\n '23569K',\n '22753H',\n '22187R',\n '23434Y',\n '22674G',\n '20674R',\n '23091E',\n '23287L',\n '22714G',\n '21452L',\n '23239Z',\n '22950F',\n '22925Q',\n '22813K',\n '21922A',\n '23242A',\n '20733F',\n '22075W',\n '21902W',\n '22628U',\n '22733V',\n '22601T',\n '22892Z',\n '21012K',\n '22465K',\n '22466O',\n '23312N',\n '23212C',\n '23215I',\n '21988M',\n '23194R',\n '23156J',\n '22211Y',\n '72351BE',\n '22594A',\n '20977U',\n '22439F',\n '23426G',\n '22207L',\n '22573R',\n '22945X',\n '85131DI',\n '48173CT',\n '22078R',\n '23237P',\n '21244H',\n '21774C',\n '22974E',\n '22812G',\n '22635R',\n '22773H',\n '21068Y',\n '22531M',\n '22120W',\n '37446S',\n '20829J',\n '37448K',\n '23130I',\n '22188M',\n '21620I',\n '22072Y',\n '22357L',\n '23365N',\n '22564D',\n '22074K',\n '22837N',\n '22584Q',\n '23291F',\n '23049U',\n '22809N',\n '22956D',\n '23032G',\n '48116K',\n '23371H',\n '22338I',\n '23366F',\n '22732E',\n '22148Z',\n '22210R',\n '21803X',\n '85049HI',\n '22807L',\n '22891F',\n '22600F',\n '23190S',\n '22805C',\n '23002D',\n '21864G',\n '21258H',\n '22938X',\n '21508L',\n '23285I',\n '22460J',\n '22915G',\n '21802W',\n '22364D',\n '22045U',\n '23288S',\n '51014CF',\n '23289K',\n '22094G',\n '22597M',\n '22603Q',\n '22842H',\n '23348D',\n '22080D',\n '21866D',\n '21158R',\n '22838C',\n '20984B',\n '22990X',\n '22675S',\n '84031AQ',\n '22581L',\n '22436B',\n '21986A',\n '23394D',\n '21870X',\n '46000SJ',\n '22744X',\n '22758C',\n '22091C',\n '22154A',\n '21619X',\n '23157X',\n '21903Y',\n '21625R',\n '22794M',\n '23378J',\n '21577X',\n '23162V',\n '22135T',\n '21713G',\n '22572W',\n '22704P',\n '21829T',\n '84828A',\n '22331U',\n '21135E',\n '23053K',\n '21527P',\n '21260A',\n '22098N',\n '16235A',\n '21773E',\n '22185B',\n '22483A',\n '23404U',\n '23102L',\n '21818Q',\n '22365S',\n '85040AS',\n '22315K',\n '72807CP',\n '23390M',\n '22209G',\n '37370Z',\n '22186K',\n '21584L',\n '21115Q',\n '10135P',\n '22678R',\n '21084K',\n '23297N',\n '22107A',\n '22244X',\n '47503AN',\n '84580O',\n '85174H',\n '23376B',\n '22278T',\n '22576Y',\n '21906V',\n '22134X',\n '21844I',\n '84912BC',\n '22975L',\n '21224S',\n '22249Z',\n '23193C',\n '22890C',\n '22686P',\n '22059M',\n '23346H',\n '23007A',\n '22375U',\n '22927Q',\n '22696M',\n '21987U',\n '22930X',\n '84971SZ',\n '21071P',\n '72760BJ',\n '85034CF',\n '22425V',\n '23096J',\n '37449B',\n '23211J',\n ...]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "a"
        }, 
        {
            "execution_count": 137, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "TypeError", 
                    "evalue": "Can not infer schema for type: <class 'str'>", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-137-17fcd0eb17fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    340\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    341\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    340\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    341\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "test_df = spark.createDataFrame(a)"
        }, 
        {
            "source": "## 4. Make New Data frame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}